/home/nab/anaconda3/envs/boltz-hackathon-niklas/lib/python3.12/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)
  _C._set_float32_matmul_precision(precision)
Checking input data.
Processing 1 inputs with 1 threads.
  0%|          | 0/1 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/home/nab/boltz-hackathon-template/src/boltz/main.py", line 583, in process_input
    raise RuntimeError(msg)  # noqa: TRY301
    ^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Missing MSA's in input and --use_msa_server flag not set.
100%|██████████| 1/1 [00:00<00:00,  1.15it/s]100%|██████████| 1/1 [00:00<00:00,  1.15it/s]
Using bfloat16 Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/nab/anaconda3/envs/boltz-hackathon-niklas/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default
Failed to process boltz_inputs/8CYH.yaml. Skipping. Error: Missing MSA's in input and --use_msa_server flag not set..
